# Generator
G_num_layers: 12
G_num_heads: 4
G_hidden_dim: 256
G_linear_key_dim: 256
G_linear_value_dim: 256
G_ffn_dim: 1024
G_dropout: 0.1
G_activation: gelu
# Discriminator
D_num_layers: 12
D_num_heads: 12
D_hidden_dim: 768
D_linear_key_dim: 768
D_linear_value_dim: 768
D_ffn_dim: 3072
D_dropout: 0.1
D_activation: gelu
char_emb_dim: 15
kernel_width: [1, 2, 3, 4, 5, 6]
kernel_depth: [32, 32, 64, 128, 256, 256]
highway_layers: 1
# Model weight
G_weight: 1.0
D_weight: 50.0
temperature: 1.0
# data config
corpus_path: data/wiki.norm.replaced.tok.en
word_vocab_path: data/bpe.en.vocab
char_vocab_path: data/char.en.vocab
model_path: char_electra.model
# training config
max_word_len: 50
max_char_len: 20
batch_size: 1024
training_steps: 100000
warmup_step: 10000
n_gpus: 8
# vocab config
mask_idx: 2
vocab_size: 30001
char_vocab_size: 251
